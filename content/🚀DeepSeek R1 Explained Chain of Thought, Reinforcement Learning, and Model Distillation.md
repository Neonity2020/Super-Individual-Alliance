---
title: "ğŸš€DeepSeek R1 Explained: Chain of Thought, Reinforcement Learning, and Model Distillation"
source: "https://medium.com/@tahirbalarabe2/deepseek-r1-explained-chain-of-thought-reinforcement-learning-and-model-distillation-0eb165d928c9"
author:
  - "[[Tahir]]"
published: 2025-01-31
created: 2025-02-07
description: "The release of DeepSeek R1, a new large language model from China, has caused a stir in the AI research community. Itâ€™s not just another incremental improvement. DeepSeek represents a significantâ€¦"
tags:
  - "clippings"
---
![[Pasted image 20250207134948.png]]
The release of DeepSeek R1, a new large language model from China, has caused a stir in the AI research community. Itâ€™s not just another incremental improvement. DeepSeek represents a significant leap forward. Most new AI models feel like small steps. [DeepSeek R1](https://github.com/deepseek-ai/DeepSeek-V3/tree/main) is different. Itâ€™s the first model in a while that makes you stop and think, *this might be important*.  
DeepSeek R1 çš„å‘å¸ƒï¼Œä½œä¸ºä¸­å›½ä¸€æ¬¾æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåœ¨äººå·¥æ™ºèƒ½ç ”ç©¶ç•Œå¼•èµ·äº†è½°åŠ¨ã€‚è¿™ä¸ä»…ä»…æ˜¯ä¸€æ¬¡å¾®å°çš„æ”¹è¿›ã€‚DeepSeek ä»£è¡¨äº†ä¸€ä¸ªé‡å¤§çš„é£è·ƒã€‚å¤§å¤šæ•°æ–°çš„ AI æ¨¡å‹éƒ½æ„Ÿè§‰åƒæ˜¯å°æ­¥ã€‚DeepSeek R1 ä¸åŒã€‚è¿™æ˜¯å¾ˆä¹…ä»¥æ¥ç¬¬ä¸€ä¸ªè®©ä½ åœä¸‹æ¥æ€è€ƒï¼Œè¿™å¯èƒ½å¾ˆé‡è¦çš„æ¨¡å‹ã€‚

A team in China released it last Sunday, and itâ€™s already making waves. Its benchmarks are close to OpenAIâ€™s 01 model in reasoning tasks â€” math, coding, and science. But whatâ€™s interesting isnâ€™t just the numbers. Itâ€™s how they got there.  
ä¸­å›½çš„ä¸€æ”¯å›¢é˜Ÿä¸Šå‘¨æ—¥å‘å¸ƒäº†å®ƒï¼Œå®ƒå·²ç»å¼•èµ·äº†è½°åŠ¨ã€‚å®ƒåœ¨æ¨ç†ä»»åŠ¡â€”â€”æ•°å­¦ã€ç¼–ç å’Œç§‘å­¦â€”â€”çš„åŸºå‡†æµ‹è¯•ä¸­æ¥è¿‘ OpenAI çš„ 01 æ¨¡å‹ã€‚ä½†æœ‰è¶£çš„ä¸åªæ˜¯æ•°å­—ã€‚è¿˜æœ‰ä»–ä»¬æ˜¯å¦‚ä½•è¾¾åˆ°è¿™ä¸ªæ°´å¹³çš„ã€‚

There are three key ideas behind [DeepSeek R1:](https://github.com/deepseek-ai/DeepSeek-V3/tree/main)  
DeepSeek R1 èƒŒåçš„ä¸‰ä¸ªå…³é”®æ€æƒ³ï¼š

1. **Chain of Thought** â€” Making the model explain itself.  
   æ€ç»´é“¾â€”â€”è®©æ¨¡å‹è§£é‡Šè‡ªå·±ã€‚
2. **Reinforcement Learning** â€” Letting it train itself.  
   å¼ºåŒ–å­¦ä¹  â€” è®©å®ƒè‡ªæˆ‘è®­ç»ƒã€‚
2. **Distillation** â€” Shrinking it without losing power.  
   è’¸é¦ â€” ç¼©å°è€Œä¸å¤±åŠ›é‡ã€‚

![[Pasted image 20250207135055.png]]
## Chain of ThoughtÂ Â æ€ç»´é“¾

If you ask most AI models a tough question, they give you an answer but not the reasoning behind it. This is a problem. If the answer is wrong, you donâ€™t know where it went off track.  
å¦‚æœä½ é—®å¤§å¤šæ•° AI æ¨¡å‹ä¸€ä¸ªéš¾é¢˜ï¼Œå®ƒä»¬ä¼šç»™ä½ ä¸€ä¸ªç­”æ¡ˆï¼Œä½†ä¸ä¼šå‘Šè¯‰ä½ èƒŒåçš„æ¨ç†ã€‚è¿™æ˜¯ä¸€ä¸ªé—®é¢˜ã€‚å¦‚æœç­”æ¡ˆé”™äº†ï¼Œä½ ä¸çŸ¥é“å®ƒåœ¨å“ªé‡Œåç¦»äº†è½¨é“ã€‚

Chain of Thought fixes this. Instead of spitting out an answer, the model explains its reasoning step by step. If it makes a mistake, you can see exactly where. More importantly, the model itself can see where.  
æ€ç»´é“¾è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æ¨¡å‹ä¸æ˜¯ç›´æ¥ç»™å‡ºç­”æ¡ˆï¼Œè€Œæ˜¯é€æ­¥è§£é‡Šå…¶æ¨ç†è¿‡ç¨‹ã€‚å¦‚æœå‡ºé”™ï¼Œä½ å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°é”™è¯¯æ‰€åœ¨ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæ¨¡å‹æœ¬èº«ä¹Ÿèƒ½çœ‹åˆ°é”™è¯¯æ‰€åœ¨ã€‚

This is more than a debugging tool. It changes how models think. The act of explaining forces them to slow down and check their own work. The result is better answers, even without extra training.  
è¿™æ˜¯ä¸€æ¬¾ä¸æ­¢æ˜¯è°ƒè¯•å·¥å…·ã€‚å®ƒæ”¹å˜äº†æ¨¡å‹æ€è€ƒçš„æ–¹å¼ã€‚è§£é‡Šçš„è¡Œä¸ºè¿«ä½¿å®ƒä»¬æ”¾æ…¢é€Ÿåº¦å¹¶æ£€æŸ¥è‡ªå·±çš„å·¥ä½œã€‚ç»“æœæ˜¯æ›´å¥½çš„ç­”æ¡ˆï¼Œç”šè‡³æ— éœ€é¢å¤–è®­ç»ƒã€‚

The DeepSeek paper shows an example with a math problem. The model walks through the solution, realizes it made a mistake, and corrects itself. Thatâ€™s new. Most AI models donâ€™t do this. They either get it right or wrong and move on.  
ã€ŠDeepSeekã€‹è®ºæ–‡å±•ç¤ºäº†ä¸€ä¸ªæ•°å­¦é—®é¢˜çš„ä¾‹å­ã€‚æ¨¡å‹èµ°è¿‡äº†è§£å†³æ–¹æ¡ˆï¼Œæ„è¯†åˆ°è‡ªå·±çŠ¯äº†ä¸€ä¸ªé”™è¯¯ï¼Œå¹¶è¿›è¡Œäº†çº æ­£ã€‚è¿™æ˜¯æ–°çš„ã€‚å¤§å¤šæ•° AI æ¨¡å‹ä¸ä¼šè¿™æ ·åšã€‚å®ƒä»¬è¦ä¹ˆåšå¯¹ï¼Œè¦ä¹ˆåšé”™ï¼Œç„¶åç»§ç»­å‰è¿›ã€‚

![](https://miro.medium.com/v2/resize:fit:1300/1*yFrza-9tmTe1I2AkFNlmOw.png)
![[Pasted image 20250207204932.png]]

[*ğŸ¤–ChatGPT for Vulnerability Detection by Tahir Balarabe  
ğŸ¤–ChatGPT ç”¨äºæ¼æ´æ£€æµ‹ï¼Œä½œè€…ï¼šTahir Balarabe*](https://medium.com/@tahirbalarabe2/chatgpt-for-vulnerability-detection-by-tahir-balarabe-affaf19bb0ad)

## Reinforcement LearningÂ Â å¼ºåŒ–å­¦ä¹ 

Most AI training looks like school: show the model a problem, give it the right answer, and repeat. DeepSeek takes a different approach. It learns more like a baby.  
å¤§å¤šæ•°äººå·¥æ™ºèƒ½è®­ç»ƒçœ‹èµ·æ¥åƒå­¦æ ¡ï¼šå‘æ¨¡å‹å±•ç¤ºä¸€ä¸ªé—®é¢˜ï¼Œç»™å®ƒæ­£ç¡®çš„ç­”æ¡ˆï¼Œç„¶åé‡å¤ã€‚DeepSeek é‡‡å–äº†ä¸€ç§ä¸åŒçš„æ–¹æ³•ã€‚å®ƒæ›´åƒæ˜¯ä¸€ä¸ªå©´å„¿é‚£æ ·å­¦ä¹ ã€‚

Babies donâ€™t get instructions. They experiment, fail, adjust, and try again. Over time, they get better. Thatâ€™s how reinforcement learning works. The model explores different ways to answer a question and picks the one that works best.  
å©´å„¿ä¸ä¼šå¾—åˆ°æŒ‡ä»¤ã€‚ä»–ä»¬å°è¯•ã€å¤±è´¥ã€è°ƒæ•´ï¼Œç„¶åå†æ¬¡å°è¯•ã€‚éšç€æ—¶é—´çš„æ¨ç§»ï¼Œä»–ä»¬å˜å¾—æ›´å¥½ã€‚è¿™å°±æ˜¯å¼ºåŒ–å­¦ä¹ çš„å·¥ä½œåŸç†ã€‚æ¨¡å‹æ¢ç´¢ä¸åŒçš„æ–¹æ³•æ¥å›ç­”ä¸€ä¸ªé—®é¢˜ï¼Œå¹¶é€‰æ‹©æœ€æœ‰æ•ˆçš„æ–¹æ³•ã€‚

This is how robots learn to walk. Itâ€™s how self-driving cars learn to navigate. And now, itâ€™s how DeepSeek improves its reasoning.  
è¿™æ˜¯æœºå™¨äººå­¦ä¹ è¡Œèµ°çš„æ–¹å¼ã€‚è¿™æ˜¯è‡ªåŠ¨é©¾é©¶æ±½è½¦å­¦ä¹ å¯¼èˆªçš„æ–¹å¼ã€‚ç°åœ¨ï¼Œè¿™æ˜¯ DeepSeek æé«˜å…¶æ¨ç†èƒ½åŠ›çš„æ–¹å¼ã€‚

The key idea is **Group Relative Policy Optimization (GRPO)**. Instead of grading answers as simply right or wrong, GRPO compares them to past attempts. If a new answer is better than an old one, the model updates its behavior.  
å…³é”®æ€æƒ³æ˜¯ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ã€‚GRPO ä¸æ˜¯ç®€å•åœ°è¯„åˆ¤ç­”æ¡ˆå¯¹é”™ï¼Œè€Œæ˜¯å°†å®ƒä»¬ä¸è¿‡å»çš„å°è¯•è¿›è¡Œæ¯”è¾ƒã€‚å¦‚æœä¸€ä¸ªæ–°ç­”æ¡ˆæ¯”æ—§ç­”æ¡ˆå¥½ï¼Œæ¨¡å‹å°±ä¼šæ›´æ–°å…¶è¡Œä¸ºã€‚

This makes learning cheaper. Instead of needing massive amounts of labeled data, the model trains itself by iterating on its own mistakes. Thatâ€™s why DeepSeek R1 improves over time while OpenAIâ€™s 01 model stays static. Given enough training, it might even reach human-level accuracy in reasoning tasks.  
è¿™ä½¿å¾—å­¦ä¹ æ›´ä¾¿å®œã€‚ä¸éœ€è¦å¤§é‡çš„æ ‡è®°æ•°æ®ï¼Œè¯¥æ¨¡å‹é€šè¿‡è¿­ä»£è‡ªå·±çš„é”™è¯¯æ¥è‡ªæˆ‘è®­ç»ƒã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ DeepSeek R1 ä¼šéšç€æ—¶é—´çš„æ¨ç§»è€Œæ”¹è¿›ï¼Œè€Œ OpenAI çš„ 01 æ¨¡å‹ä¿æŒé™æ€ã€‚åœ¨è¶³å¤Ÿçš„è®­ç»ƒä¸‹ï¼Œå®ƒç”šè‡³å¯èƒ½åœ¨æ¨ç†ä»»åŠ¡ä¸­è¾¾åˆ°äººç±»æ°´å¹³çš„å‡†ç¡®æ€§ã€‚

![[Pasted image 20250207205357.png]]

[*ğŸ¤–ChatGPT for Vulnerability Detection by Tahir Balarabe  
ğŸ¤–ChatGPT ç”¨äºæ¼æ´æ£€æµ‹ï¼Œä½œè€…ï¼šTahir Balarabe*](https://medium.com/@tahirbalarabe2/chatgpt-for-vulnerability-detection-by-tahir-balarabe-affaf19bb0ad)

## DistillationÂ Â è’¸é¦

Thereâ€™s a problem with models like DeepSeek: theyâ€™re too big.  
æ·±åº¦å¯»æ±‚ç­‰æ¨¡å‹å­˜åœ¨é—®é¢˜ï¼šå®ƒä»¬å¤ªå¤§ã€‚

The full version has 671 billion parameters. Running it requires thousands of GPUs and the kind of infrastructure only tech giants can afford. That makes it impractical for most people.  
å®Œæ•´ç‰ˆæœ¬æœ‰ 6710 äº¿ä¸ªå‚æ•°ã€‚è¿è¡Œå®ƒéœ€è¦æ•°åƒä¸ª GPU ä»¥åŠåªæœ‰ç§‘æŠ€å·¨å¤´æ‰èƒ½è´Ÿæ‹…å¾—èµ·çš„åŸºç¡€è®¾æ–½ã€‚è¿™ä½¿å¾—å®ƒå¯¹å¤§å¤šæ•°äººæ¥è¯´ä¸åˆ‡å®é™…ã€‚

The solution is **distillation** â€” taking a giant model and compressing it into a smaller one without losing too much performance. Think of it as teaching an apprentice. The big model generates examples, and the small model learns from them.  
è§£å†³æ–¹æ¡ˆæ˜¯è’¸é¦â€”â€”å°†ä¸€ä¸ªå¤§å‹æ¨¡å‹å‹ç¼©æˆä¸€ä¸ªæ›´å°çš„æ¨¡å‹ï¼ŒåŒæ—¶ä¸æŸå¤±å¤ªå¤šæ€§èƒ½ã€‚æŠŠå®ƒæƒ³è±¡æˆæ•™å¯¼å­¦å¾’ã€‚å¤§å‹æ¨¡å‹ç”Ÿæˆç¤ºä¾‹ï¼Œå°å‹æ¨¡å‹ä»ä¸­å­¦ä¹ ã€‚

DeepSeek researchers distilled their model into Llama 3 and Qwen. The surprising part? The smaller models sometimes performed better than the original. This makes AI far more accessible. Instead of needing a supercomputer, you can run a powerful model on a single GPU.  
DeepSeek ç ”ç©¶äººå‘˜å°†ä»–ä»¬çš„æ¨¡å‹æç‚¼æˆäº† Llama 3 å’Œ Qwenã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œè¾ƒå°çš„æ¨¡å‹æœ‰æ—¶ç”šè‡³æ¯”åŸå§‹æ¨¡å‹è¡¨ç°æ›´å¥½ã€‚è¿™ä½¿å¾—äººå·¥æ™ºèƒ½å˜å¾—æ›´åŠ æ˜“äºè®¿é—®ã€‚ä¸å†éœ€è¦è¶…çº§è®¡ç®—æœºï¼Œä½ å¯ä»¥åœ¨å•ä¸ª GPU ä¸Šè¿è¡Œä¸€ä¸ªå¼ºå¤§çš„æ¨¡å‹ã€‚

![](https://miro.medium.com/v2/resize:fit:1300/1*PINQ3ZBjxMoHkwWNFK7Llg.png)

[*ğŸ¤–ChatGPT for Vulnerability Detection by Tahir Balarabe  
ğŸ¤–ChatGPT ç”¨äºæ¼æ´æ£€æµ‹ï¼Œä½œè€…ï¼šTahir Balarabe*](https://medium.com/@tahirbalarabe2/chatgpt-for-vulnerability-detection-by-tahir-balarabe-affaf19bb0ad)

## Why This MattersÂ Â ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

DeepSeekâ€™s combination of Chain of Thought reasoning, reinforcement learning, and model distillation makes it a formidable tool. Itâ€™s not just about raw power. Itâ€™s about creating models that are accurate, transparent, and accessible.  
DeepSeek ç»“åˆäº†æ€ç»´é“¾æ¨ç†ã€å¼ºåŒ–å­¦ä¹ å’Œæ¨¡å‹è’¸é¦ï¼Œä½¿å…¶æˆä¸ºä¸€æ¬¾å¼ºå¤§çš„å·¥å…·ã€‚è¿™ä¸ä»…ä»…å…³ä¹åŸå§‹åŠ›é‡ã€‚è¿™å…³ä¹åˆ›å»ºå‡†ç¡®ã€é€æ˜å’Œæ˜“äºè®¿é—®çš„æ¨¡å‹ã€‚

Chain of Thought makes the modelâ€™s reasoning clear. Reinforcement learning allows it to improve over time. And distillation ensures that these capabilities are available to a wider audience, not just those with access to supercomputers.  
æ€ç»´é“¾ä½¿æ¨¡å‹çš„æ¨ç†æ¸…æ™°ã€‚å¼ºåŒ–å­¦ä¹ ä½¿å®ƒèƒ½å¤Ÿéšç€æ—¶é—´çš„æ¨ç§»è€Œæ”¹è¿›ã€‚è€Œè’¸é¦ç¡®ä¿è¿™äº›èƒ½åŠ›å¯ä¾›æ›´å¹¿æ³›çš„å—ä¼—ä½¿ç”¨ï¼Œè€Œä¸ä»…ä»…æ˜¯é‚£äº›èƒ½å¤Ÿè®¿é—®è¶…çº§è®¡ç®—æœºçš„äººã€‚

If youâ€™re interested in AI, DeepSeek is worth paying attention to. Itâ€™s not just another incremental improvement. Itâ€™s a step toward models that can think, learn, and adapt in ways that were previously out of reach.  
å¦‚æœæ‚¨å¯¹ AI æ„Ÿå…´è¶£ï¼ŒDeepSeek å€¼å¾—å…³æ³¨ã€‚å®ƒä¸ä»…ä»…æ˜¯å¦ä¸€ä¸ªæ¸è¿›å¼æ”¹è¿›ã€‚å®ƒæ˜¯ä¸€æ­¥å‘èƒ½å¤Ÿä»¥å…ˆå‰æ— æ³•è§¦åŠçš„æ–¹å¼æ€è€ƒã€å­¦ä¹ å’Œé€‚åº”çš„æ¨¡å‹è¿ˆè¿›ã€‚

The best part? You donâ€™t need to be an AI researcher to see its potential. The techniques behind DeepSeek are already being applied in real-world applications, from coding assistants to scientific research tools. And as these models become more accessible, their impact will only grow.  
æœ€å¥½çš„éƒ¨åˆ†ï¼Ÿä½ ä¸éœ€è¦æˆä¸ºä¸€å AI ç ”ç©¶äººå‘˜å°±èƒ½çœ‹åˆ°å®ƒçš„æ½œåŠ›ã€‚DeepSeek èƒŒåçš„æŠ€æœ¯å·²ç»åœ¨å®é™…åº”ç”¨ä¸­å¾—åˆ°äº†åº”ç”¨ï¼Œä»ç¼–ç åŠ©æ‰‹åˆ°ç§‘å­¦ç ”ç©¶å·¥å…·ã€‚éšç€è¿™äº›æ¨¡å‹å˜å¾—æ›´åŠ æ˜“äºè·å–ï¼Œå®ƒä»¬çš„å½±å“åªä¼šå¢é•¿ã€‚

DeepSeek R1 is important not just because of what it can do, but because of how it does it.  
DeepSeek R1 ä¹‹æ‰€ä»¥é‡è¦ï¼Œä¸ä»…å› ä¸ºå®ƒèƒ½åšä»€ä¹ˆï¼Œè¿˜å› ä¸ºå®ƒæ˜¯å¦‚ä½•åšåˆ°çš„ã€‚

- Chain of Thought makes AI more transparent.  
  æ€ç»´é“¾ä½¿äººå·¥æ™ºèƒ½æ›´åŠ é€æ˜ã€‚
- Reinforcement learning makes it more self-improving.  
  å¼ºåŒ–å­¦ä¹ ä½¿å…¶æ›´å…·è‡ªæˆ‘æ”¹è¿›èƒ½åŠ›ã€‚
- Distillation makes it more available.  
  è’¸é¦ä½¿å…¶æ›´æ˜“è·å¾—ã€‚

These arenâ€™t just optimizations. Theyâ€™re shifts in how AI models work. And if DeepSeek keeps improving, it might push the entire field forward.  
è¿™ä¸æ˜¯ç®€å•çš„ä¼˜åŒ–ã€‚è¿™æ˜¯ AI æ¨¡å‹å·¥ä½œæ–¹å¼çš„è½¬å˜ã€‚å¦‚æœ DeepSeek æŒç»­æ”¹è¿›ï¼Œå®ƒå¯èƒ½ä¼šæ¨åŠ¨æ•´ä¸ªé¢†åŸŸå‘å‰å‘å±•ã€‚

If you want to see where AI is going, this is a good place to look.  
å¦‚æœæ‚¨æƒ³äº†è§£äººå·¥æ™ºèƒ½çš„å‘å±•æ–¹å‘ï¼Œè¿™é‡Œæ˜¯ä¸€ä¸ªä¸é”™çš„è§‚å¯Ÿç‚¹ã€‚

So, if youâ€™re curious, dive into the [*paper*](https://github.com/deepseek-ai/DeepSeek-V3/tree/main). Or better yet, try out DeepSeek for yourself. Itâ€™s not every day that you get to see a breakthrough in action.  
æ‰€ä»¥ï¼Œå¦‚æœä½ å¥½å¥‡ï¼Œå°±æ·±å…¥ç ”ç©¶è¿™ç¯‡è®ºæ–‡å§ã€‚æˆ–è€…æ›´å¥½çš„æ˜¯ï¼Œäº²è‡ªå°è¯•ä¸€ä¸‹ DeepSeekã€‚ä¸æ˜¯æ¯å¤©éƒ½èƒ½çœ‹åˆ°çªç ´æ€§è¿›å±•çš„å®é™…åº”ç”¨ã€‚

[*ğŸ¤–ChatGPT for Vulnerability Detection by Tahir Balarabe  
ğŸ¤–ChatGPT ç”¨äºæ¼æ´æ£€æµ‹ï¼Œä½œè€…ï¼šTahir Balarabe*](https://medium.com/@tahirbalarabe2/chatgpt-for-vulnerability-detection-by-tahir-balarabe-affaf19bb0ad)

![](https://miro.medium.com/v2/resize:fit:1400/1*YsBPZhuF2qIZX8N9kZ1jYQ.png)

## Further Reading::Â Â è¿›ä¸€æ­¥é˜…è¯»ï¼š

[ğŸ¤–ChatGPT for Vulnerability Detection by Tahir Balarabe  
ğŸ¤–ChatGPT ç”¨äºæ¼æ´æ£€æµ‹ï¼Œä½œè€…ï¼šTahir Balarabe](https://medium.com/@tahirbalarabe2/chatgpt-for-vulnerability-detection-by-tahir-balarabe-affaf19bb0ad)

[What are AI Agents?  
äººå·¥æ™ºèƒ½ä»£ç†æ˜¯ä»€ä¹ˆï¼Ÿ](https://medium.com/@tahirbalarabe2/what-are-ai-agents-f06ef775e78f)

[Stable Diffusion Deepfakes: Creation and Detection  
ç¨³å®šæ‰©æ•£æ·±åº¦ä¼ªé€ ï¼šåˆ›å»ºä¸æ£€æµ‹](https://medium.com/@tahirbalarabe2/stable-diffusion-deepfakes-creation-and-detection-15103f99f55d)

[The Difference Between AI Assistants and AI Agents (And Why It Matters)  
äººå·¥æ™ºèƒ½åŠ©æ‰‹ä¸äººå·¥æ™ºèƒ½ä»£ç†çš„åŒºåˆ«ï¼ˆä»¥åŠä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦ï¼‰](https://medium.com/@tahirbalarabe2/stable-diffusion-deepfakes-creation-and-detection-15103f99f55d)

[ğŸ¤”What is AI Inferencing?  
äººå·¥æ™ºèƒ½æ¨ç†æ˜¯ä»€ä¹ˆï¼Ÿ](http://xn--what%20is%20ai%20inferencing-i7171a/?)

[ğŸ’¡Prompt Tuning: A New Approach to Large Language Model Specialization  
æç¤ºå¾®è°ƒï¼šå¤§å‹è¯­è¨€æ¨¡å‹ä¸“ä¸šåŒ–çš„æ–°æ–¹æ³•](https://medium.com/@tahirbalarabe2/prompt-tuning-a-new-approach-to-language-model-specialization-6abd12cea528)

[ğŸ’¡Retrieval-Augmented Generation(RAG) for Accurate LLMs  
æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ç”¨äºå‡†ç¡®LLMs](http://xn--retrieval-augmented%20generation\(rag\)%20for%20accurate%20llms-lb495d/)

## FAQ about DeepSeek R1Â Â å…³äº DeepSeek R1 çš„å¸¸è§é—®é¢˜

**What is DeepSeek R1 and why is it significant?  
ä»€ä¹ˆæ˜¯ DeepSeek R1 ä»¥åŠä¸ºä»€ä¹ˆå®ƒå¾ˆé‡è¦ï¼Ÿ**

DeepSeek R1 is a new large language model developed by a research team in China. Itâ€™s significant because it demonstrates performance comparable to leading models like OpenAIâ€™s 01 on complex tasks like mathematical, coding, and scientific reasoning. The modelâ€™s innovations, particularly in using reinforcement learning and model distillation, could potentially make AI more efficient and accessible.  
DeepSeek R1 æ˜¯ä¸­å›½ç ”ç©¶å›¢é˜Ÿå¼€å‘çš„æ–°å¤§å‹è¯­è¨€æ¨¡å‹ã€‚å®ƒçš„é‡è¦æ€§åœ¨äºï¼Œå®ƒåœ¨æ•°å­¦ã€ç¼–ç å’Œç§‘å­¦æ¨ç†ç­‰å¤æ‚ä»»åŠ¡ä¸Šçš„è¡¨ç°å¯ä¸ OpenAI çš„ 01 ç­‰é¢†å…ˆæ¨¡å‹ç›¸åª²ç¾ã€‚è¯¥æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ å’Œæ¨¡å‹è’¸é¦æ–¹é¢çš„åˆ›æ–°ï¼Œæœ‰å¯èƒ½ä½¿äººå·¥æ™ºèƒ½æ›´åŠ é«˜æ•ˆå’Œæ˜“äºè®¿é—®ã€‚

**How does DeepSeek R1 use Chain of Thought prompting, and what benefits does it provide?  
DeepSeek R1 å¦‚ä½•ä½¿ç”¨æ€ç»´é“¾æç¤ºï¼Œå®ƒæä¾›äº†å“ªäº›å¥½å¤„ï¼Ÿ**

DeepSeek R1 uses Chain of Thought prompting by encouraging the model to â€œthink out loudâ€ or provide step-by-step reasoning in its responses. For example, when solving math problems, it will show each step of its work. This approach not only allows for identifying mistakes more easily but also makes it possible for the model to self-evaluate and improve its accuracy through re-prompting or re-evaluation of its steps.  
DeepSeek R1 é€šè¿‡é¼“åŠ±æ¨¡å‹â€œå¤§å£°æ€è€ƒâ€æˆ–åœ¨å…¶å›ç­”ä¸­æä¾›é€æ­¥æ¨ç†ï¼Œä½¿ç”¨æ€ç»´é“¾æç¤ºã€‚ä¾‹å¦‚ï¼Œåœ¨è§£å†³æ•°å­¦é—®é¢˜æ—¶ï¼Œå®ƒä¼šå±•ç¤ºå…¶å·¥ä½œçš„æ¯ä¸€æ­¥ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æ›´å®¹æ˜“è¯†åˆ«é”™è¯¯ï¼Œè€Œä¸”ä½¿æ¨¡å‹èƒ½å¤Ÿé€šè¿‡é‡æ–°æç¤ºæˆ–é‡æ–°è¯„ä¼°å…¶æ­¥éª¤æ¥è‡ªæˆ‘è¯„ä¼°å¹¶æé«˜å…¶å‡†ç¡®æ€§ã€‚

**How does DeepSeek R1 employ reinforcement learning, and how does it differ from typical methods?  
DeepSeek R1 å¦‚ä½•ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œä»¥åŠå®ƒä¸å…¸å‹æ–¹æ³•æœ‰ä½•ä¸åŒï¼Ÿ**

DeepSeek R1 uses reinforcement learning to learn through self-guided exploration, similar to how a baby learns to walk. Instead of being trained with explicit question-answer pairs, it explores its â€œenvironmentâ€ and optimizes its behavior by maximizing rewards, for example, by favoring shorter, more efficient methods when solving equations. This differs from traditional methods where models are explicitly trained with input/output pairs. A key difference is that DeepSeek R1â€™s performance increases over time rather than remaining static.  
DeepSeek R1 é€šè¿‡è‡ªæˆ‘å¼•å¯¼æ¢ç´¢å­¦ä¹ ï¼Œç±»ä¼¼äºå©´å„¿å­¦ä¹ èµ°è·¯çš„æ–¹å¼ã€‚å®ƒä¸æ˜¯é€šè¿‡æ˜¾å¼çš„é—®ç­”å¯¹è¿›è¡Œè®­ç»ƒï¼Œè€Œæ˜¯æ¢ç´¢å…¶â€œç¯å¢ƒâ€ï¼Œé€šè¿‡æœ€å¤§åŒ–å¥–åŠ±æ¥ä¼˜åŒ–å…¶è¡Œä¸ºï¼Œä¾‹å¦‚ï¼Œåœ¨è§£æ–¹ç¨‹æ—¶ä¼˜å…ˆé€‰æ‹©æ›´çŸ­ã€æ›´æœ‰æ•ˆçš„æ–¹æ³•ã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œä¼ ç»Ÿæ–¹æ³•ä¸­æ¨¡å‹æ˜¯é€šè¿‡è¾“å…¥/è¾“å‡ºå¯¹è¿›è¡Œæ˜¾å¼è®­ç»ƒçš„ã€‚ä¸€ä¸ªå…³é”®çš„åŒºåˆ«æ˜¯ï¼ŒDeepSeek R1 çš„æ€§èƒ½éšç€æ—¶é—´çš„æ¨ç§»è€Œæé«˜ï¼Œè€Œä¸æ˜¯ä¿æŒé™æ€ã€‚

**What is Group Relative Policy Optimization (GRPO) and how does it work within DeepSeek R1?  
ä»€ä¹ˆæ˜¯ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ä»¥åŠå®ƒå¦‚ä½•åœ¨ DeepSeek R1 ä¸­å·¥ä½œï¼Ÿ**

Group Relative Policy Optimization (GRPO) is a reinforcement learning technique used by DeepSeek R1 to self-improve by comparing new responses with previous ones. It assigns a reward based on the relative improvement from past responses. To prevent drastic changes in behavior, it uses a clipping function to ensure stability while maximizing the modelâ€™s reward, enabling gradual refinement and optimization of the model over time.  
ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ˜¯ DeepSeek R1 ä½¿ç”¨çš„å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼Œé€šè¿‡æ¯”è¾ƒæ–°æ—§å“åº”æ¥è‡ªæˆ‘æ”¹è¿›ã€‚å®ƒæ ¹æ®è¿‡å»å“åº”çš„ç›¸å¯¹æ”¹è¿›åˆ†é…å¥–åŠ±ã€‚ä¸ºäº†é˜²æ­¢è¡Œä¸ºå‘ç”Ÿå‰§çƒˆå˜åŒ–ï¼Œå®ƒä½¿ç”¨è£å‰ªå‡½æ•°æ¥ç¡®ä¿ç¨³å®šæ€§ï¼ŒåŒæ—¶æœ€å¤§åŒ–æ¨¡å‹çš„å¥–åŠ±ï¼Œä½¿æ¨¡å‹éšç€æ—¶é—´çš„æ¨ç§»é€æ¸ä¼˜åŒ–å’Œæ”¹è¿›ã€‚

**What is model distillation, and why is it important in the context of DeepSeek R1?  
ä»€ä¹ˆæ˜¯æ¨¡å‹è’¸é¦ï¼Œä»¥åŠä¸ºä»€ä¹ˆå®ƒåœ¨ DeepSeek R1 çš„èƒŒæ™¯ä¸‹å¾ˆé‡è¦ï¼Ÿ**

Model distillation is the process of transferring knowledge from a large, complex model (like DeepSeek R1 with its 671 billion parameters) to a smaller, more lightweight model (like Llama 3 or Qwen). This makes the technology more accessible because it reduces the computational resources needed to run the model. Interestingly, the smaller models sometimes even outperform the original larger model.  
æ¨¡å‹è’¸é¦æ˜¯å°†çŸ¥è¯†ä»å¤§å‹ã€å¤æ‚æ¨¡å‹ï¼ˆå¦‚æ‹¥æœ‰ 6710 äº¿å‚æ•°çš„ DeepSeek R1ï¼‰è½¬ç§»åˆ°æ›´å°ã€æ›´è½»é‡çº§æ¨¡å‹ï¼ˆå¦‚ Llama 3 æˆ– Qwenï¼‰çš„è¿‡ç¨‹ã€‚è¿™ä½¿å¾—æŠ€æœ¯æ›´åŠ æ˜“äºè®¿é—®ï¼Œå› ä¸ºå®ƒå‡å°‘äº†è¿è¡Œæ¨¡å‹æ‰€éœ€çš„è®¡ç®—èµ„æºã€‚æœ‰è¶£çš„æ˜¯ï¼Œè¾ƒå°çš„æ¨¡å‹æœ‰æ—¶ç”šè‡³èƒ½è¶…è¶ŠåŸå§‹çš„å¤§å‹æ¨¡å‹ã€‚

**How does model distillation benefit the accessibility of AI technology?  
æ¨¡å‹è’¸é¦å¦‚ä½•ä½¿äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¯åŠæ€§å—ç›Šï¼Ÿ**

Model distillation makes high-performance AI more accessible by allowing researchers to create smaller language models that operate at a fraction of the cost without a significant reduction in performance. This enables wider use of AI technologies as it doesnâ€™t require huge computing infrastructures. This opens the door for smaller teams and individuals to run very capable LLMs.  
æ¨¡å‹è’¸é¦é€šè¿‡å…è®¸ç ”ç©¶äººå‘˜åˆ›å»ºæ€§èƒ½é™ä½å¾ˆå°ä½†æˆæœ¬ä»…ä¸ºåŸæ¥ä¸€å°éƒ¨åˆ†çš„æ›´å°è¯­è¨€æ¨¡å‹ï¼Œä½¿å¾—é«˜æ€§èƒ½ AI æ›´åŠ æ˜“äºè·å–ã€‚è¿™ä½¿å¾— AI æŠ€æœ¯çš„åº”ç”¨èŒƒå›´æ›´å¹¿ï¼Œå› ä¸ºå®ƒä¸éœ€è¦åºå¤§çš„è®¡ç®—åŸºç¡€è®¾æ–½ã€‚è¿™ä¸ºå°å‹å›¢é˜Ÿå’Œä¸ªäººè¿è¡Œéå¸¸å¼ºå¤§çš„LLMsæ‰“å¼€äº†å¤§é—¨ã€‚

**How does the combination of Chain of Thought prompting, reinforcement learning, and GRPO contribute to DeepSeek R1â€™s overall performance?  
å¦‚ä½•å°†æ€ç»´é“¾æç¤ºã€å¼ºåŒ–å­¦ä¹ å’Œ GRPO çš„ç»“åˆå¯¹ DeepSeek R1 çš„æ•´ä½“æ€§èƒ½åšå‡ºè´¡çŒ®ï¼Ÿ**

By combining Chain of Thought prompting and reinforcement learning with GRPO, DeepSeek R1 achieves its high level of performance and self-improvement. Chain of Thought allows the model to self-reflect on its reasoning, while reinforcement learning enables it to optimize its approach based on the rewards it earns from its performance. GRPO stabilizes the learning process by incrementally comparing new responses to older ones, allowing it to make more efficient and less erratic improvements.  
é€šè¿‡ç»“åˆæ€ç»´é“¾æç¤ºå’Œ GRPO å¼ºåŒ–å­¦ä¹ ï¼ŒDeepSeek R1 å®ç°äº†å…¶é«˜æ€§èƒ½å’Œè‡ªæˆ‘æå‡ã€‚æ€ç»´é“¾ä½¿æ¨¡å‹èƒ½å¤Ÿå¯¹è‡ªå·±çš„æ¨ç†è¿›è¡Œè‡ªæˆ‘åæ€ï¼Œè€Œå¼ºåŒ–å­¦ä¹ åˆ™ä½¿å®ƒèƒ½å¤Ÿæ ¹æ®ä»å…¶è¡¨ç°ä¸­è·å¾—çš„å¥–åŠ±æ¥ä¼˜åŒ–å…¶æ–¹æ³•ã€‚GRPO é€šè¿‡é€æ­¥æ¯”è¾ƒæ–°å“åº”ä¸æ—§å“åº”æ¥ç¨³å®šå­¦ä¹ è¿‡ç¨‹ï¼Œä½¿å…¶èƒ½å¤Ÿè¿›è¡Œæ›´é«˜æ•ˆå’Œæ›´ç¨³å®šçš„æ”¹è¿›ã€‚

**What are the main takeaways from the research behind DeepSeek R1?  
æ·±åº¦ Seek R1 èƒŒåçš„ç ”ç©¶æœ‰å“ªäº›ä¸»è¦æ”¶è·ï¼Ÿ**

The main takeaways from the research behind DeepSeek R1 are its utilization of Chain of Thought reasoning for improved accuracy, reinforcement learning with GRPO for self-optimization and increased performance over time, and model distillation to improve accessibility to powerful AI without requiring vast computational resources. These three innovations represent progress toward more efficient, accessible, and scalable large language models.  
æ·±åº¦ Seek R1 ç ”ç©¶çš„ä¸»è¦æ”¶è·æ˜¯å…¶åˆ©ç”¨æ€ç»´é“¾æ¨ç†æé«˜å‡†ç¡®æ€§ï¼Œä½¿ç”¨ GRPO è¿›è¡Œå¼ºåŒ–å­¦ä¹ ä»¥å®ç°è‡ªæˆ‘ä¼˜åŒ–å’Œæ€§èƒ½éšæ—¶é—´æå‡ï¼Œä»¥åŠæ¨¡å‹è’¸é¦ä»¥åœ¨ä¸éœ€å¤§é‡è®¡ç®—èµ„æºçš„æƒ…å†µä¸‹æé«˜å¼ºå¤§ AI çš„æ˜“ç”¨æ€§ã€‚è¿™ä¸‰ä¸ªåˆ›æ–°ä»£è¡¨äº†å‘æ›´é«˜æ•ˆã€æ˜“ç”¨å’Œå¯æ‰©å±•çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿ˆè¿›ã€‚